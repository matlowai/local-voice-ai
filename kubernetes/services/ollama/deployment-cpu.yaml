---
# CPU-Only Ollama Deployment (Fallback Option)
# Used when GPU is not available or as backup

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-cpu
  namespace: voice-ai
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm-service
    app.kubernetes.io/part-of: local-voice-ai
    app.kubernetes.io/version: "v1.0.0"
    deployment-type: "cpu-fallback"
  annotations:
    description: "CPU-only Ollama LLM service for Local Voice AI (fallback)"
    fallback-for: "ollama-gpu"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
      deployment-type: "cpu-fallback"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/component: llm-service
        app.kubernetes.io/part-of: local-voice-ai
        deployment-type: "cpu-fallback"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "11434"
        prometheus.io/path: "/metrics"
    spec:
      # Node selector for CPU-optimized nodes
      nodeSelector:
        cpu: "high"
        memory: "high"
      tolerations:
      - key: "cpu-only"
        operator: "Exists"
        effect: "NoSchedule"
      
      # Service account
      serviceAccountName: ollama-cpu
      
      containers:
      - name: ollama
        image: ollama/ollama:latest
        imagePullPolicy: IfNotPresent
        
        # CPU and memory resource allocation (higher than GPU version)
        resources:
          limits:
            cpu: "12000m"
            memory: "32Gi"
          requests:
            cpu: "6000m"
            memory: "20Gi"
        
        # Environment variables
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0"
        - name: OLLAMA_PORT
          value: "11434"
        - name: OLLAMA_ORIGINS
          value: "*"
        - name: OLLAMA_MODELS
          valueFrom:
            configMapKeyRef:
              name: ollama-config
              key: OLLAMA_MODELS
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "1"  # Reduced for CPU
        - name: OLLAMA_NUM_PARALLEL
          value: "2"  # Reduced for CPU
        - name: OLLAMA_TIMEOUT
          valueFrom:
            configMapKeyRef:
              name: ollama-config
              key: OLLAMA_TIMEOUT
        - name: OLLAMA_LOAD_TIMEOUT
          value: "20m"  # Increased for CPU
        - name: OLLAMA_CPU_ONLY
          value: "true"
        - name: OLLAMA_LOW_VRAM
          value: "true"
        - name: OLLAMA_NUM_THREAD
          value: "8"  # Optimize for multi-core CPU
        - name: GOMAXPROCS
          value: "8"
        
        # Ports
        ports:
        - name: http
          containerPort: 11434
          protocol: TCP
        
        # Health checks (longer delays for CPU)
        livenessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 60
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 3
        
        startupProbe:
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 40
        
        # Volume mounts
        volumeMounts:
        - name: ollama-storage
          mountPath: /root/.ollama
        - name: temp-storage
          mountPath: /tmp
        - name: cpu-config
          mountPath: /etc/cpu-config
          readOnly: true
        
        # Security context
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
      
      # Volumes
      volumes:
      - name: ollama-storage
        persistentVolumeClaim:
          claimName: ollama-storage
      - name: temp-storage
        persistentVolumeClaim:
          claimName: temp-storage
      - name: cpu-config
        configMap:
          name: cpu-config
      
      # Restart policy
      restartPolicy: Always
      
      # Termination grace period
      terminationGracePeriodSeconds: 180

---
# Service Account for CPU Ollama
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ollama-cpu
  namespace: voice-ai
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: service-account
    app.kubernetes.io/part-of: local-voice-ai
automountServiceAccountToken: false

---
# ConfigMap for CPU-specific configurations
apiVersion: v1
kind: ConfigMap
metadata:
  name: cpu-config
  namespace: voice-ai
  labels:
    app.kubernetes.io/name: cpu-config
    app.kubernetes.io/component: config
    app.kubernetes.io/part-of: local-voice-ai
data:
  config.yaml: |
    # CPU-specific optimizations
    
    # Model settings
    cpu_models:
      - "gemma3:4b"
      - "qwen:4b"
      - "phi3:mini"
    
    # Performance settings
    cpu_optimization:
      num_threads: 8
      max_concurrent_requests: 2
      batch_size: 1
      context_size: 2048
    
    # Memory management
    memory_management:
      swap_usage: "enabled"
      memory_limit: "28Gi"
      gc_frequency: "high"
    
    # Thermal management
    thermal_management:
      max_cpu_temp: 85
      cpu_throttle: true
      cooling_policy: "performance"
  
  cpu-optimization.sh: |
    #!/bin/bash
    # CPU optimization script for Ollama
    
    echo "Optimizing CPU settings for Ollama..."
    
    # Set CPU governor to performance
    for cpu in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do
      echo performance | sudo tee $cpu
    done
    
    # Optimize process scheduling
    echo 'kernel.sched_migration_cost_ns = 5000000' | sudo sysctl -p
    
    # Increase file limits
    ulimit -n 65536
    
    # Optimize memory management
    echo 'vm.swappiness = 10' | sudo sysctl -p
    echo 'vm.dirty_ratio = 15' | sudo sysctl -p
    echo 'vm.dirty_background_ratio = 5' | sudo sysctl -p
    
    echo "CPU optimization completed"
    
    # Monitor CPU usage
    watch -n 1 'top -bn1 | grep "Cpu\|ollama"'
  
  model-loader-cpu.sh: |
    #!/bin/bash
    # Preload models for CPU inference
    
    MODELS=("gemma3:4b" "qwen:4b")
    
    echo "Preloading CPU models..."
    
    for model in "${MODELS[@]}"; do
      echo "Loading model: $model"
      ollama pull "$model"
      if [ $? -eq 0 ]; then
        echo "Successfully loaded: $model"
      else
        echo "Failed to load: $model"
      fi
    done
    
    echo "CPU model preloading completed"
    ollama list
  
  health-check-cpu.sh: |
    #!/bin/bash
    # Health check script for CPU Ollama
    
    # Check if Ollama is responding
    if curl -f http://localhost:11434/api/tags > /dev/null 2>&1; then
      echo "Ollama API is responding"
    else
      echo "Ollama API is not responding"
      exit 1
    fi
    
    # Check CPU usage
    CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    echo "CPU Usage: ${CPU_USAGE}%"
    
    # Check memory usage
    MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')
    echo "Memory Usage: ${MEMORY_USAGE}%"
    
    # Check load average
    LOAD_AVG=$(uptime | awk -F'load average:' '{print $2}')
    echo "Load Average: $LOAD_AVG"
    
    # Check loaded models
    echo "Loaded models:"
    ollama list
    
    echo "CPU health check completed successfully"

---
# Horizontal Pod Autoscaler for CPU deployment
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ollama-cpu-hpa
  namespace: voice-ai
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: autoscaling
    app.kubernetes.io/part-of: local-voice-ai
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ollama-cpu
  minReplicas: 1
  maxReplicas: 3  # Can scale CPU instances
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 20
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60

---
# Pod Disruption Budget for CPU deployment
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ollama-cpu-pdb
  namespace: voice-ai
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: pdb
    app.kubernetes.io/part-of: local-voice-ai
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
      deployment-type: "cpu-fallback"

---
# Service that points to active deployment (GPU or CPU)
apiVersion: v1
kind: Service
metadata:
  name: ollama-active
  namespace: voice-ai
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: service
    app.kubernetes.io/part-of: local-voice-ai
  annotations:
    description: "Active Ollama service (switches between GPU and CPU)"
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 11434
    targetPort: 11434
    protocol: TCP
  selector:
    # This will be updated by the switch script
    app.kubernetes.io/name: ollama
    # deployment-type will be toggled between gpu-optimized and cpu-fallback